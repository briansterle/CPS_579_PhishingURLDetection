{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df436217",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from datasets     import Dataset, DatasetDict\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1a359d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nobell.it/70ffb52d079109dca5664cce6f317373782/...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>www.dghjdgf.com/paypal.co.uk/cycgi-bin/webscrc...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>serviciosbys.com/paypal.cgi.bin.get-into.herf....</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mail.printakid.com/www.online.americanexpress....</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>thewhiskeydregs.com/wp-content/themes/widescre...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 URL  label\n",
       "0  nobell.it/70ffb52d079109dca5664cce6f317373782/...      1\n",
       "1  www.dghjdgf.com/paypal.co.uk/cycgi-bin/webscrc...      1\n",
       "2  serviciosbys.com/paypal.cgi.bin.get-into.herf....      1\n",
       "3  mail.printakid.com/www.online.americanexpress....      1\n",
       "4  thewhiskeydregs.com/wp-content/themes/widescre...      1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"../data/phishing_site_urls.csv\")\n",
    "df['label'] = df['Label'].map({'good': 0, 'bad': 1})\n",
    "df = df[['URL', 'label']]\n",
    "df.head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78b3c886",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, stratify=df['label'], random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2781eb5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 439476/439476 [00:09<00:00, 45896.54 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 109870/109870 [00:02<00:00, 44021.96 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer  = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"URL\"],\n",
    "                     truncation=True,\n",
    "                     padding=\"max_length\",\n",
    "                     max_length=128)\n",
    "\n",
    "ds = DatasetDict({\n",
    "    \"train\": Dataset.from_pandas(train_df),\n",
    "    \"test\":  Dataset.from_pandas(test_df),\n",
    "})\n",
    "\n",
    "ds = ds.rename_column(\"label\", \"labels\")\n",
    "\n",
    "ds = ds.map(tokenize, batched=True)\n",
    "ds.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b098d7f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "322ba042",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/anaconda3/envs/m4pytorch/lib/python3.12/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  if isinstance(self.fsdp_config.get(\"transformer_layer_cls_to_wrap\", None), str):\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Using the `Trainer` with `PyTorch` requires `accelerate>=0.26.0`: Please run `pip install transformers[torch]` or `pip install 'accelerate>=0.26.0'`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 18\u001b[0m\n\u001b[1;32m      5\u001b[0m     args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[1;32m      6\u001b[0m         output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./checkpoints/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      7\u001b[0m         per_device_train_batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m         logging_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m,\n\u001b[1;32m     12\u001b[0m     )\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Trainer(model\u001b[38;5;241m=\u001b[39mmodel, args\u001b[38;5;241m=\u001b[39margs,\n\u001b[1;32m     14\u001b[0m                    train_dataset\u001b[38;5;241m=\u001b[39mds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     15\u001b[0m                    eval_dataset\u001b[38;5;241m=\u001b[39mds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     16\u001b[0m                    tokenizer\u001b[38;5;241m=\u001b[39mtokenizer)\n\u001b[0;32m---> 18\u001b[0m distil_trainer \u001b[38;5;241m=\u001b[39m make_trainer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdistilbert-base-uncased\u001b[39m\u001b[38;5;124m\"\u001b[39m, ds)\n\u001b[1;32m     19\u001b[0m bert_trainer   \u001b[38;5;241m=\u001b[39m make_trainer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbert-base-uncased\u001b[39m\u001b[38;5;124m\"\u001b[39m,     ds)\n\u001b[1;32m     21\u001b[0m distil_trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
      "Cell \u001b[0;32mIn[12], line 5\u001b[0m, in \u001b[0;36mmake_trainer\u001b[0;34m(model_name, ds)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmake_trainer\u001b[39m(model_name, ds):\n\u001b[1;32m      4\u001b[0m     model \u001b[38;5;241m=\u001b[39m AutoModelForSequenceClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name, num_labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m     args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[1;32m      6\u001b[0m         output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./checkpoints/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      7\u001b[0m         per_device_train_batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m,\n\u001b[1;32m      8\u001b[0m         per_device_eval_batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m,\n\u001b[1;32m      9\u001b[0m         evaluation_strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     10\u001b[0m         num_train_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[1;32m     11\u001b[0m         logging_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m,\n\u001b[1;32m     12\u001b[0m     )\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Trainer(model\u001b[38;5;241m=\u001b[39mmodel, args\u001b[38;5;241m=\u001b[39margs,\n\u001b[1;32m     14\u001b[0m                    train_dataset\u001b[38;5;241m=\u001b[39mds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     15\u001b[0m                    eval_dataset\u001b[38;5;241m=\u001b[39mds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     16\u001b[0m                    tokenizer\u001b[38;5;241m=\u001b[39mtokenizer)\n",
      "File \u001b[0;32m<string>:134\u001b[0m, in \u001b[0;36m__init__\u001b[0;34m(self, output_dir, overwrite_output_dir, do_train, do_eval, do_predict, eval_strategy, prediction_loss_only, per_device_train_batch_size, per_device_eval_batch_size, per_gpu_train_batch_size, per_gpu_eval_batch_size, gradient_accumulation_steps, eval_accumulation_steps, eval_delay, torch_empty_cache_steps, learning_rate, weight_decay, adam_beta1, adam_beta2, adam_epsilon, max_grad_norm, num_train_epochs, max_steps, lr_scheduler_type, lr_scheduler_kwargs, warmup_ratio, warmup_steps, log_level, log_level_replica, log_on_each_node, logging_dir, logging_strategy, logging_first_step, logging_steps, logging_nan_inf_filter, save_strategy, save_steps, save_total_limit, save_safetensors, save_on_each_node, save_only_model, restore_callback_states_from_checkpoint, no_cuda, use_cpu, use_mps_device, seed, data_seed, jit_mode_eval, use_ipex, bf16, fp16, fp16_opt_level, half_precision_backend, bf16_full_eval, fp16_full_eval, tf32, local_rank, ddp_backend, tpu_num_cores, tpu_metrics_debug, debug, dataloader_drop_last, eval_steps, dataloader_num_workers, dataloader_prefetch_factor, past_index, run_name, disable_tqdm, remove_unused_columns, label_names, load_best_model_at_end, metric_for_best_model, greater_is_better, ignore_data_skip, fsdp, fsdp_min_num_params, fsdp_config, fsdp_transformer_layer_cls_to_wrap, accelerator_config, deepspeed, label_smoothing_factor, optim, optim_args, adafactor, group_by_length, length_column_name, report_to, ddp_find_unused_parameters, ddp_bucket_cap_mb, ddp_broadcast_buffers, dataloader_pin_memory, dataloader_persistent_workers, skip_memory_metrics, use_legacy_prediction_loop, push_to_hub, resume_from_checkpoint, hub_model_id, hub_strategy, hub_token, hub_private_repo, hub_always_push, gradient_checkpointing, gradient_checkpointing_kwargs, include_inputs_for_metrics, include_for_metrics, eval_do_concat_batches, fp16_backend, evaluation_strategy, push_to_hub_model_id, push_to_hub_organization, push_to_hub_token, mp_parameters, auto_find_batch_size, full_determinism, torchdynamo, ray_scope, ddp_timeout, torch_compile, torch_compile_backend, torch_compile_mode, dispatch_batches, split_batches, include_tokens_per_second, include_num_input_tokens_seen, neftune_noise_alpha, optim_target_modules, batch_eval_metrics, eval_on_start, use_liger_kernel, eval_use_gather_object, average_tokens_across_devices)\u001b[0m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/m4pytorch/lib/python3.12/site-packages/transformers/training_args.py:1791\u001b[0m, in \u001b[0;36m__post_init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1788\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m   1789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mddp_timeout_delta\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m timedelta:\n\u001b[1;32m   1790\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1791\u001b[0m \u001b[38;5;124;03m    The actual timeout for torch.distributed.init_process_group since it expects a timedelta variable.\u001b[39;00m\n\u001b[1;32m   1792\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   1793\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m timedelta(seconds\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mddp_timeout)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/m4pytorch/lib/python3.12/site-packages/transformers/training_args.py:2313\u001b[0m, in \u001b[0;36mdevice\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2279\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mset_testing\u001b[39m(\n\u001b[1;32m   2280\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   2281\u001b[0m     batch_size: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m8\u001b[39m,\n\u001b[1;32m   2282\u001b[0m     loss_only: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   2283\u001b[0m     jit_mode: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   2284\u001b[0m ):\n\u001b[1;32m   2285\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2286\u001b[0m \u001b[38;5;124;03m    A method that regroups all basic arguments linked to testing on a held-out dataset.\u001b[39;00m\n\u001b[1;32m   2287\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2311\u001b[0m \u001b[38;5;124;03m    ```\u001b[39;00m\n\u001b[1;32m   2312\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2313\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_predict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   2314\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mper_device_eval_batch_size \u001b[38;5;241m=\u001b[39m batch_size\n\u001b[1;32m   2315\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_loss_only \u001b[38;5;241m=\u001b[39m loss_only\n",
      "File \u001b[0;32m/opt/anaconda3/envs/m4pytorch/lib/python3.12/site-packages/transformers/utils/generic.py:62\u001b[0m, in \u001b[0;36m__get__\u001b[0;34m(self, obj, objtype)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstrtobool\u001b[39m(val):\n\u001b[1;32m     61\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Convert a string representation of truth to true (1) or false (0).\u001b[39;00m\n\u001b[0;32m---> 62\u001b[0m \n\u001b[1;32m     63\u001b[0m \u001b[38;5;124;03m    True values are 'y', 'yes', 't', 'true', 'on', and '1'; false values are 'n', 'no', 'f', 'false', 'off', and '0'.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;124;03m    Raises ValueError if 'val' is anything else.\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m     66\u001b[0m     val \u001b[38;5;241m=\u001b[39m val\u001b[38;5;241m.\u001b[39mlower()\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m val \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myes\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m}:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/m4pytorch/lib/python3.12/site-packages/transformers/training_args.py:2186\u001b[0m, in \u001b[0;36m_setup_devices\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2143\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mset_training\u001b[39m(\n\u001b[1;32m   2144\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   2145\u001b[0m     learning_rate: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5e-5\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2152\u001b[0m     gradient_checkpointing: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   2153\u001b[0m ):\n\u001b[1;32m   2154\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2155\u001b[0m \u001b[38;5;124;03m    A method that regroups all basic arguments linked to the training.\u001b[39;00m\n\u001b[1;32m   2156\u001b[0m \n\u001b[1;32m   2157\u001b[0m \u001b[38;5;124;03m    <Tip>\u001b[39;00m\n\u001b[1;32m   2158\u001b[0m \n\u001b[1;32m   2159\u001b[0m \u001b[38;5;124;03m    Calling this method will automatically set `self.do_train` to `True`.\u001b[39;00m\n\u001b[1;32m   2160\u001b[0m \n\u001b[1;32m   2161\u001b[0m \u001b[38;5;124;03m    </Tip>\u001b[39;00m\n\u001b[1;32m   2162\u001b[0m \n\u001b[1;32m   2163\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   2164\u001b[0m \u001b[38;5;124;03m        learning_rate (`float`, *optional*, defaults to 5e-5):\u001b[39;00m\n\u001b[1;32m   2165\u001b[0m \u001b[38;5;124;03m            The initial learning rate for the optimizer.\u001b[39;00m\n\u001b[1;32m   2166\u001b[0m \u001b[38;5;124;03m        batch_size (`int` *optional*, defaults to 8):\u001b[39;00m\n\u001b[1;32m   2167\u001b[0m \u001b[38;5;124;03m            The batch size per device (GPU/TPU core/CPU...) used for training.\u001b[39;00m\n\u001b[1;32m   2168\u001b[0m \u001b[38;5;124;03m        weight_decay (`float`, *optional*, defaults to 0):\u001b[39;00m\n\u001b[1;32m   2169\u001b[0m \u001b[38;5;124;03m            The weight decay to apply (if not zero) to all layers except all bias and LayerNorm weights in the\u001b[39;00m\n\u001b[1;32m   2170\u001b[0m \u001b[38;5;124;03m            optimizer.\u001b[39;00m\n\u001b[1;32m   2171\u001b[0m \u001b[38;5;124;03m        num_train_epochs(`float`, *optional*, defaults to 3.0):\u001b[39;00m\n\u001b[1;32m   2172\u001b[0m \u001b[38;5;124;03m            Total number of training epochs to perform (if not an integer, will perform the decimal part percents\u001b[39;00m\n\u001b[1;32m   2173\u001b[0m \u001b[38;5;124;03m            of the last epoch before stopping training).\u001b[39;00m\n\u001b[1;32m   2174\u001b[0m \u001b[38;5;124;03m        max_steps (`int`, *optional*, defaults to -1):\u001b[39;00m\n\u001b[1;32m   2175\u001b[0m \u001b[38;5;124;03m            If set to a positive number, the total number of training steps to perform. Overrides\u001b[39;00m\n\u001b[1;32m   2176\u001b[0m \u001b[38;5;124;03m            `num_train_epochs`. In case of using a finite iterable dataset the training may stop before reaching\u001b[39;00m\n\u001b[1;32m   2177\u001b[0m \u001b[38;5;124;03m            the set number of steps when all data is exhausted.\u001b[39;00m\n\u001b[1;32m   2178\u001b[0m \u001b[38;5;124;03m        gradient_accumulation_steps (`int`, *optional*, defaults to 1):\u001b[39;00m\n\u001b[1;32m   2179\u001b[0m \u001b[38;5;124;03m            Number of updates steps to accumulate the gradients for, before performing a backward/update pass.\u001b[39;00m\n\u001b[1;32m   2180\u001b[0m \n\u001b[1;32m   2181\u001b[0m \u001b[38;5;124;03m            <Tip warning={true}>\u001b[39;00m\n\u001b[1;32m   2182\u001b[0m \n\u001b[1;32m   2183\u001b[0m \u001b[38;5;124;03m            When using gradient accumulation, one step is counted as one step with backward pass. Therefore,\u001b[39;00m\n\u001b[1;32m   2184\u001b[0m \u001b[38;5;124;03m            logging, evaluation, save will be conducted every `gradient_accumulation_steps * xxx_step` training\u001b[39;00m\n\u001b[1;32m   2185\u001b[0m \u001b[38;5;124;03m            examples.\u001b[39;00m\n\u001b[0;32m-> 2186\u001b[0m \n\u001b[1;32m   2187\u001b[0m \u001b[38;5;124;03m            </Tip>\u001b[39;00m\n\u001b[1;32m   2188\u001b[0m \n\u001b[1;32m   2189\u001b[0m \u001b[38;5;124;03m        seed (`int`, *optional*, defaults to 42):\u001b[39;00m\n\u001b[1;32m   2190\u001b[0m \u001b[38;5;124;03m            Random seed that will be set at the beginning of training. To ensure reproducibility across runs, use\u001b[39;00m\n\u001b[1;32m   2191\u001b[0m \u001b[38;5;124;03m            the [`~Trainer.model_init`] function to instantiate the model if it has some randomly initialized\u001b[39;00m\n\u001b[1;32m   2192\u001b[0m \u001b[38;5;124;03m            parameters.\u001b[39;00m\n\u001b[1;32m   2193\u001b[0m \u001b[38;5;124;03m        gradient_checkpointing (`bool`, *optional*, defaults to `False`):\u001b[39;00m\n\u001b[1;32m   2194\u001b[0m \u001b[38;5;124;03m            If True, use gradient checkpointing to save memory at the expense of slower backward pass.\u001b[39;00m\n\u001b[1;32m   2195\u001b[0m \n\u001b[1;32m   2196\u001b[0m \u001b[38;5;124;03m    Example:\u001b[39;00m\n\u001b[1;32m   2197\u001b[0m \n\u001b[1;32m   2198\u001b[0m \u001b[38;5;124;03m    ```py\u001b[39;00m\n\u001b[1;32m   2199\u001b[0m \u001b[38;5;124;03m    >>> from transformers import TrainingArguments\u001b[39;00m\n\u001b[1;32m   2200\u001b[0m \n\u001b[1;32m   2201\u001b[0m \u001b[38;5;124;03m    >>> args = TrainingArguments(\"working_dir\")\u001b[39;00m\n\u001b[1;32m   2202\u001b[0m \u001b[38;5;124;03m    >>> args = args.set_training(learning_rate=1e-4, batch_size=32)\u001b[39;00m\n\u001b[1;32m   2203\u001b[0m \u001b[38;5;124;03m    >>> args.learning_rate\u001b[39;00m\n\u001b[1;32m   2204\u001b[0m \u001b[38;5;124;03m    1e-4\u001b[39;00m\n\u001b[1;32m   2205\u001b[0m \u001b[38;5;124;03m    ```\u001b[39;00m\n\u001b[1;32m   2206\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   2207\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_train \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   2208\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlearning_rate \u001b[38;5;241m=\u001b[39m learning_rate\n",
      "\u001b[0;31mImportError\u001b[0m: Using the `Trainer` with `PyTorch` requires `accelerate>=0.26.0`: Please run `pip install transformers[torch]` or `pip install 'accelerate>=0.26.0'`"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "def make_trainer(model_name, ds):\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "    args = TrainingArguments(\n",
    "        output_dir=f\"./checkpoints/{model_name.split('/')[-1]}\",\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=32,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        num_train_epochs=3,\n",
    "        logging_steps=50,\n",
    "    )\n",
    "    return Trainer(model=model, args=args,\n",
    "                   train_dataset=ds[\"train\"],\n",
    "                   eval_dataset=ds[\"test\"],\n",
    "                   tokenizer=tokenizer)\n",
    "\n",
    "distil_trainer = make_trainer(\"distilbert-base-uncased\", ds)\n",
    "bert_trainer   = make_trainer(\"bert-base-uncased\",     ds)\n",
    "\n",
    "distil_trainer.train()\n",
    "bert_trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a48b15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d5c31df2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate in /opt/anaconda3/envs/m4pytorch/lib/python3.12/site-packages (1.6.0)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /opt/anaconda3/envs/m4pytorch/lib/python3.12/site-packages (from accelerate) (2.0.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/envs/m4pytorch/lib/python3.12/site-packages (from accelerate) (24.2)\n",
      "Requirement already satisfied: psutil in /opt/anaconda3/envs/m4pytorch/lib/python3.12/site-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: pyyaml in /opt/anaconda3/envs/m4pytorch/lib/python3.12/site-packages (from accelerate) (6.0.2)\n",
      "Requirement already satisfied: torch>=2.0.0 in /opt/anaconda3/envs/m4pytorch/lib/python3.12/site-packages (from accelerate) (2.5.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /opt/anaconda3/envs/m4pytorch/lib/python3.12/site-packages (from accelerate) (0.30.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/anaconda3/envs/m4pytorch/lib/python3.12/site-packages (from accelerate) (0.5.3)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/envs/m4pytorch/lib/python3.12/site-packages (from huggingface-hub>=0.21.0->accelerate) (3.17.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/envs/m4pytorch/lib/python3.12/site-packages (from huggingface-hub>=0.21.0->accelerate) (2025.3.2)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/envs/m4pytorch/lib/python3.12/site-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/anaconda3/envs/m4pytorch/lib/python3.12/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/envs/m4pytorch/lib/python3.12/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.12.2)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/envs/m4pytorch/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (75.8.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/anaconda3/envs/m4pytorch/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/envs/m4pytorch/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/envs/m4pytorch/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/envs/m4pytorch/lib/python3.12/site-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/m4pytorch/lib/python3.12/site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/m4pytorch/lib/python3.12/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/m4pytorch/lib/python3.12/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/m4pytorch/lib/python3.12/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/m4pytorch/lib/python3.12/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2025.4.26)\n"
     ]
    }
   ],
   "source": [
    "!pip install accelerate --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a151d8",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (563591931.py, line 3)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31m``\u001b[39m\n    ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "def eval_trainer(trainer):\n",
    "    preds_output = trainer.predict(ds[\"test\"])\n",
    "    preds = np.argmax(preds_output.predictions, axis=1)\n",
    "    true  = preds_output.label_ids\n",
    "    print(classification_report(true, preds))\n",
    "    return true, preds\n",
    "\n",
    "true_d, pred_d = eval_trainer(distil_trainer)\n",
    "true_b, pred_b = eval_trainer(bert_trainer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5edc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a batch\n",
    "batch = ds[\"test\"][0]\n",
    "inputs = { k: batch[k].unsqueeze(0) for k in [\"input_ids\",\"attention_mask\"] }\n",
    "model = bert_trainer.model.eval()\n",
    "\n",
    "# forward pass with outputs\n",
    "outputs = model(**inputs, output_attentions=True)\n",
    "attns   = outputs.attentions  # tuple: one tensor per layer\n",
    "\n",
    "# pick layer 0, head 0\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(attns[0][0,0].detach().numpy())\n",
    "plt.title(\"Layer 0, Head 0\")\n",
    "plt.xlabel(\"Token position\")\n",
    "plt.ylabel(\"Token position\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e627118f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Confusion matrices\n",
    "for name, (t, p) in [(\"DistilBERT\", (true_d,pred_d)),\n",
    "                    (\"BERT\",     (true_b,pred_b))]:\n",
    "    disp = ConfusionMatrixDisplay.from_predictions(t, p, normalize=\"true\")\n",
    "    disp.figure_.suptitle(f\"{name} Confusion Matrix\")\n",
    "\n",
    "# Accuracy bar plot\n",
    "accs = [accuracy_score(true_d, pred_d), accuracy_score(true_b, pred_b)]\n",
    "plt.figure()\n",
    "plt.bar([\"DistilBERT\",\"BERT\"], accs)\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Model Comparison\")\n",
    "plt.ylim(0,1)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "m4pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
